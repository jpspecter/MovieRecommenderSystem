{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.wsd import lesk\n",
    "#from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.parse.corenlp import CoreNLPDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "#print(stop_words)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization(sentence):\n",
    "    tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains hypernyms\n",
    "def WordNetHypernyms(sentence, word_tokens):\n",
    "    # Creates dictionary for hypernyms\n",
    "    hypernyms_list = []\n",
    "    \n",
    "    # Populates the above dictionaries according to the word senses associated with them\n",
    "    for token in word_tokens:\n",
    "        # Extracts best sense for each word using LESK\n",
    "        best_sense = lesk(sentence, token)\n",
    "        # token_synset = wn.synset(token)\n",
    "        # hypernyms_list.append(token_synset.hypernyms()[0].lemmas()[0].name())\n",
    "        \n",
    "        if best_sense is not None:\n",
    "            # Obtains Hypernyms\n",
    "            if best_sense.hypernyms() != []:\n",
    "                hypernyms_list.append(best_sense.hypernyms()[0].lemmas()[0].name())\n",
    "            \n",
    "    return hypernyms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_Pipeline(sentence):\n",
    "    word_tokens = Tokenization(sentence)\n",
    "    hypernyms = WordNetHypernyms(sentence, word_tokens)\n",
    "    return hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A soldier-of-fortune steals some Russian nerve gas from Afghanistan, and brings it to the U.S. to be analyzed. A greedy millionaire rancher finds out about it and sets out to steal it.\n",
      "['gain', 'native', 'brace', 'fossil_fuel', 'change', 'rich_person', 'farmer', 'pronounce', 'representation', 'gain']\n"
     ]
    }
   ],
   "source": [
    "# sample_text = \"A soldier-of-fortune steals some Russian nerve gas from Afghanistan, and brings it to the U.S. to be analyzed. A greedy millionaire rancher finds out about it and sets out to steal it.\"\n",
    "# print(sample_text)\n",
    "# print(NLP_Pipeline(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_list = ['2012', 'A Beautiful Mind', 'Amadeus', 'Avatar', 'Clash of the Titans', 'Les Miserables', 'Star Wars', 'The Expendables', 'The Godfather', 'The Matrix Revolutions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dict = {}\n",
    "movies_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in movie_list:\n",
    "    folder_name = 'Data/' + i\n",
    "    review_content = ''\n",
    "    hypernyms_list = []\n",
    "    for num in range(1, 100):\n",
    "        #print('read file: ' + str(num))\n",
    "        file_name = folder_name + \"/\" + str(num) + \".txt\"\n",
    "        with open(file_name) as f:\n",
    "            file_contents = f.read()\n",
    "            review_content += file_contents\n",
    "    hypernyms_list = NLP_Pipeline(review_content)\n",
    "    movies_dict[i] = hypernyms_list\n",
    "    for hypernym in hypernyms_list:\n",
    "        #print(hypernym)\n",
    "        counts_dict[hypernym] = counts_dict.get(hypernym, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "counts_dict_sorted = sorted(counts_dict, key=counts_dict.get, reverse=True)\n",
    "top_100_counts = counts_dict_sorted[0:100]\n",
    "print(len(top_100_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_concepts_array = np.zeros(shape=(len(movies_dict), len(top_100_counts)))\n",
    "row_number = -1\n",
    "for movie in movies_dict:\n",
    "    row_number = row_number + 1\n",
    "    col_number = -1\n",
    "    concepts_list = movies_dict[movie]\n",
    "    for concept in top_100_counts:\n",
    "        col_number = col_number + 1\n",
    "        if concept in concepts_list:\n",
    "            movies_concepts_array[row_number][col_number] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(movie_num_1, movie_num_2):\n",
    "    result = 1- spatial.distance.cosine(movies_concepts_array[movie_num_1], movies_concepts_array[movie_num_2])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix = np.zeros(shape=(len(movies_dict), 100))\n",
    "\n",
    "for i in range(len(movie_list)):\n",
    "    file_name = 'Data/' + movie_list[i] + '/rating.txt'\n",
    "    with open(file_name) as f:\n",
    "        file_contents = f.read()\n",
    "        file_contents = file_contents.split(' = ')\n",
    "        file_contents = file_contents[1].split(' , ')\n",
    "        for j in range(100):\n",
    "            if j == 0:\n",
    "                temp = file_contents[j].split(' ')\n",
    "                ratings_matrix[i][j] = temp[1]\n",
    "            elif j == 99:\n",
    "                temp = file_contents[j].split(' ')\n",
    "                ratings_matrix[i][j] = temp[0]\n",
    "            else:\n",
    "                ratings_matrix[i][j] = file_contents[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratings_prediction(movie_name, user_number):\n",
    "    if movie_name not in movie_list:\n",
    "        return None\n",
    "    if user_number > len(ratings_matrix[0]):\n",
    "        return None\n",
    "    \n",
    "    for i in range(len(movie_list)):\n",
    "        if movie_name == movie_list[i]:\n",
    "            movie_number = i\n",
    "\n",
    "    pred_rating = 0\n",
    "\n",
    "    for i in range(len(movie_list)):\n",
    "        if movie_list[i] == movie_name:\n",
    "            continue\n",
    "        else:\n",
    "            #print(str(cosine_similarity(movie_number, i)) + \"\\t\" + str(ratings_matrix[i][user_number - 1]))\n",
    "            pred_rating = pred_rating + cosine_similarity(movie_number, i) * ratings_matrix[i][user_number - 1]\n",
    "    \n",
    "    return float(pred_rating/ (len(movie_list)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9794334923752579\n"
     ]
    }
   ],
   "source": [
    "# VERY HIGH SIMILARITY VALUE\n",
    "movie_id_1 = 1              # A Beautiful Mind\n",
    "movie_id_2 = 7              # The Expendables\n",
    "print(cosine_similarity(1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_prediction('2012', 5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12203f4631f1c8a04127b7540e373e2eec32b1de98b5a481febbb709a77e5f64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
